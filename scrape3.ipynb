{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by R. David Beales for the [Kelvin Smith Library](https://case.edu/library/) at [Case Western Reserve University](https://case.edu) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email rdb104@case.edu.<br />\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping: Making a Request and Receiving a Response\n",
    "\n",
    "**Description:** This lesson explores how to collect multiple connected data points from a web page and store them in a csv file.  \n",
    "\n",
    "**Use Case:** For Learners (Additional explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "\n",
    "**Completion time:** 60 minutes\n",
    "\n",
    "**Knowledge Required:** Basic Python\n",
    "\n",
    "**Knowledge Recommended:** HTML Structure\n",
    "\n",
    "**Data Format:** `html`, `txt`, `py` \n",
    "\n",
    "**Libraries Used:** `requests` \n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project #3: Scraping sets of related information into CSV files.\n",
    "\n",
    "Build a scraper that collects multiple data points about each book based upon specific criteria.\n",
    "\n",
    "In this project you will:\n",
    "1. Determine what data we are able to collect about each book listed in the store.  \n",
    "2. Use the `Inspect` tool in your web browser to identify the web page struture for those pieces of data. \n",
    "3. Understand and use a python script to crawl the web page and extract only the data that meets the classification criteria we identified in steps 1 and 2.\n",
    "4. Write the data to a csv file. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What data is available?\n",
    "\n",
    "If we look at this screenshot captured from the web page, we can see that there are several intresting pieces of data in addition to the title.  There is the price, the rating, and whether or not the book is in stock.  We could also save the cover images or collect the links to those images, but let's leave those out for right now.  So we are going to try to collect four pieces of data for each book; title, price, rating, stock status.\n",
    "\n",
    "![title](img/booklisting.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll get started the same way we have in the last few lessons, importing packages! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests  #https://requests.readthedocs.io/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, just like before, use requests to get the content of the website, store it in a variable, and then use BeautifulSoup to parse that content into the \"soup\" we can analyze.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Fetch the page\n",
    "results = requests.get(\"https://books.toscrape.com/\")\n",
    "\n",
    "# 2.Get the page content and assign it to the varaible 'content'\n",
    "content = results.text\n",
    "\n",
    "# 3. Create the soup\n",
    "soup = BeautifulSoup(content, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the html structure of the page so we can determine how we can identify each piece of information to scrape.  \n",
    "\n",
    "Each book's information is presenteed in an `article` element with the `class=product_pod`.  We can use `find_all` to find all of these `article` elements and then scrape the data we need from each one.  \n",
    "\n",
    "`articles = soup.find_all('article', class_='product_pod')`\n",
    "\n",
    "As we determinded in the last lesson, the title is contained in the `h3` element.  \n",
    "\n",
    "`title = article.find('h3').find('a')['title']`\n",
    "\n",
    "We can see the price data is contained in a `p` element with `class=price_color`.    \n",
    "\n",
    "`price = article.find('p', class_='price_color').text`\n",
    "\n",
    "The rating is contained in another `p` element with `class=\"star-rating NUMBER\"`.  We don't want the whole `p` element, just the `class`, so we add `['class']` to this line to limit what we scrape.\n",
    "\n",
    "`rating = article.find('p', class_='star-rating')['class']`\n",
    "\n",
    "For stock status, we are using the same apporach of identifying the element by its `class`.  \n",
    "\n",
    "`stock = article.find('p', class_='instock availability').text`\n",
    "\n",
    "\n",
    "![title](img/htmlscrape.png)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A Light in the Attic', 'Â£51.77', ['star-rating', 'Three'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "['Tipping the Velvet', 'Â£53.74', ['star-rating', 'One'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "['Soumission', 'Â£50.10', ['star-rating', 'One'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "['Sharp Objects', 'Â£47.82', ['star-rating', 'Four'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "['Sapiens: A Brief History of Humankind', 'Â£54.23', ['star-rating', 'Five'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "['The Requiem Red', 'Â£22.65', ['star-rating', 'One'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "['The Dirty Little Secrets of Getting Your Dream Job', 'Â£33.34', ['star-rating', 'Four'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "['The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull', 'Â£17.93', ['star-rating', 'Three'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "['The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics', 'Â£22.60', ['star-rating', 'Four'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "['The Black Maria', 'Â£52.15', ['star-rating', 'One'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "['Starving Hearts (Triangular Trade Trilogy, #1)', 'Â£13.99', ['star-rating', 'Two'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "[\"Shakespeare's Sonnets\", 'Â£20.66', ['star-rating', 'Four'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "['Set Me Free', 'Â£17.46', ['star-rating', 'Five'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "[\"Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\", 'Â£52.29', ['star-rating', 'Five'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "['Rip it Up and Start Again', 'Â£35.02', ['star-rating', 'Five'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "['Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991', 'Â£57.25', ['star-rating', 'Three'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "['Olio', 'Â£23.88', ['star-rating', 'One'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "['Mesaerion: The Best Science Fiction Stories 1800-1849', 'Â£37.59', ['star-rating', 'One'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "['Libertarianism for Beginners', 'Â£51.33', ['star-rating', 'Two'], '\\n\\n    \\n        In stock\\n    \\n']\n",
      "[\"It's Only the Himalayas\", 'Â£45.17', ['star-rating', 'Two'], '\\n\\n    \\n        In stock\\n    \\n']\n"
     ]
    }
   ],
   "source": [
    "# Find all article elements with class 'product_pod'\n",
    "articles = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "# Initialize an empty list to store book information\n",
    "book_info_list = []\n",
    "\n",
    "# Iterate through each article to extract information and store in a list\n",
    "for article in articles:\n",
    "    # Extract title\n",
    "    title = article.find('h3').find('a')['title']\n",
    "\n",
    "    # Extract product price\n",
    "    price = article.find('p', class_='price_color').text\n",
    "\n",
    "    # Extract star rating (if available)\n",
    "    rating = article.find('p', class_='star-rating')['class']\n",
    "\n",
    "    # Extract stock status\n",
    "    stock = article.find('p', class_='instock availability').text\n",
    "\n",
    "    # Store the information in a list\n",
    "    book_info = [title, price, rating, stock]\n",
    "\n",
    "    # Append the book information to the main list\n",
    "    book_info_list.append(book_info)\n",
    "\n",
    "# Print the list of lists containing book information\n",
    "for book_info in book_info_list:\n",
    "    print(book_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also using `.strip()` on the text.  You can specify which leading and trailing characters you'd like to remove from a string. In our case, we've left the parentheses empty so the strip method will use the default argument, which is just to remove any white space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A Light in the Attic', '£51.77', 'Three', 'In stock']\n",
      "['Tipping the Velvet', '£53.74', 'One', 'In stock']\n",
      "['Soumission', '£50.10', 'One', 'In stock']\n",
      "['Sharp Objects', '£47.82', 'Four', 'In stock']\n",
      "['Sapiens: A Brief History of Humankind', '£54.23', 'Five', 'In stock']\n",
      "['The Requiem Red', '£22.65', 'One', 'In stock']\n",
      "['The Dirty Little Secrets of Getting Your Dream Job', '£33.34', 'Four', 'In stock']\n",
      "['The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull', '£17.93', 'Three', 'In stock']\n",
      "['The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics', '£22.60', 'Four', 'In stock']\n",
      "['The Black Maria', '£52.15', 'One', 'In stock']\n",
      "['Starving Hearts (Triangular Trade Trilogy, #1)', '£13.99', 'Two', 'In stock']\n",
      "[\"Shakespeare's Sonnets\", '£20.66', 'Four', 'In stock']\n",
      "['Set Me Free', '£17.46', 'Five', 'In stock']\n",
      "[\"Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\", '£52.29', 'Five', 'In stock']\n",
      "['Rip it Up and Start Again', '£35.02', 'Five', 'In stock']\n",
      "['Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991', '£57.25', 'Three', 'In stock']\n",
      "['Olio', '£23.88', 'One', 'In stock']\n",
      "['Mesaerion: The Best Science Fiction Stories 1800-1849', '£37.59', 'One', 'In stock']\n",
      "['Libertarianism for Beginners', '£51.33', 'Two', 'In stock']\n",
      "[\"It's Only the Himalayas\", '£45.17', 'Two', 'In stock']\n"
     ]
    }
   ],
   "source": [
    "# Find all article elements with class 'product_pod'\n",
    "articles = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "# Initialize an empty list to store book information\n",
    "book_info_list = []\n",
    "\n",
    "# Iterate through each article to extract information and store in a list\n",
    "for article in articles:\n",
    "    # Extract title\n",
    "    title = article.find('h3').find('a')['title']\n",
    "\n",
    "    # Extract product price\n",
    "    price = article.find('p', class_='price_color').text.replace(\"Â\", \"\")\n",
    "    \n",
    "    # Extract star rating (if available)\n",
    "    rating = article.find('p', class_='star-rating')['class'][1]\n",
    "\n",
    "    # Extract stock status\n",
    "    stock = article.find('p', class_='instock availability').text.strip()\n",
    "\n",
    "    # Store the information in a list\n",
    "    book_info = [title, price, rating, stock]\n",
    "\n",
    "    # Append the book information to the main list\n",
    "    book_info_list.append(book_info)\n",
    "\n",
    "# Print the list of lists containing book information\n",
    "for book_info in book_info_list:\n",
    "    print(book_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all article elements with class 'product_pod'\n",
    "articles = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "# Initialize an empty list to store book information\n",
    "book_info_list = []\n",
    "\n",
    "# Iterate through each article to extract information and store in a list\n",
    "for article in articles:\n",
    "    # Extract title\n",
    "    title = article.find('h3').find('a')['title']\n",
    "\n",
    "    # Extract product price\n",
    "    price = article.find('p', class_='price_color').text.replace('Â', '')\n",
    "\n",
    "    # Extract star rating (if available)\n",
    "    rating = article.find('p', class_='star-rating')['class'][1]\n",
    "\n",
    "    # Extract stock status\n",
    "    stock = article.find('p', class_='instock availability').text.strip()\n",
    "\n",
    "    # Store the information in a list\n",
    "    book_info = [title, price, rating, stock]\n",
    "\n",
    "    # Append the book information to the main list\n",
    "    book_info_list.append(book_info)\n",
    "\n",
    "# Print the list of lists containing book information\n",
    "for book_info in book_info_list:\n",
    "    print(book_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scrape.txt','w') as file:\n",
    "    file.write(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
