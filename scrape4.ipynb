{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by R. David Beales for the [Kelvin Smith Library](https://case.edu/library/) at [Case Western Reserve University](https://case.edu) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email rdb104@case.edu.<br />\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping: Making a Request and Receiving a Response\n",
    "\n",
    "**Description:** This lesson introduces the basic web scraping workflow using the `requests` library for Python.  \n",
    "\n",
    "**Use Case:** For Learners (Additional explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "\n",
    "**Completion time:** 15 minutes\n",
    "\n",
    "**Knowledge Required:** Basic Python\n",
    "\n",
    "**Knowledge Recommended:** HTML Structure\n",
    "\n",
    "**Data Format:** `html`, `txt`, `py` \n",
    "\n",
    "**Libraries Used:** `requests` \n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Data from Multiple Pages\n",
    "\n",
    "In this project you will:\n",
    "1. Use the `Inspect` tool to explore how <a href=\"https://books.toscrape.com/\">Books to Scrape</a> handles navigation between pages.\n",
    "2. Understand and use a python script to direct the web scraper to a navigate to the next page if there is one, and scrape the specified data from there, and repeat this process until there are no more pages.\n",
    "3. Examine the data to see if our scraper worked the way we think it should.\n",
    "4. Write the data to a csv file. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping multiple pages using `requests`.\n",
    "\n",
    "We're going to be using much of the same code we used in the last lesson, as the data we are trying to collect is the same.  However, we are going to wrap that code in some navigational instructions for \n",
    "\n",
    "\n",
    "![title](img/next.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests  #https://requests.readthedocs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Fetch the page\n",
    "results = requests.get(\"https://books.toscrape.com/\")\n",
    "\n",
    "# 2.Get the page content and assign it to the varaible 'content'\n",
    "content = results.text\n",
    "\n",
    "# 3. Create the soup\n",
    "soup = BeautifulSoup(content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all article elements with class 'product_pod'\n",
    "articles = soup.find_all('article', class_='product_pod')\n",
    "\n",
    "# Initialize an empty list to store book information\n",
    "book_info_list = []\n",
    "\n",
    "# Iterate through each article to extract information and store in a list\n",
    "for article in articles:\n",
    "    # Extract title\n",
    "    title = article.find('h3').find('a')['title']\n",
    "\n",
    "    # Extract product price\n",
    "    price = article.find('p', class_='price_color').text.replace(\"Â\", \"\")\n",
    "    \n",
    "    # Extract star rating (if available)\n",
    "    rating = article.find('p', class_='star-rating')['class'][1]\n",
    "\n",
    "    # Extract stock status\n",
    "    stock = article.find('p', class_='instock availability').text.strip()\n",
    "\n",
    "    # Store the information in a list\n",
    "    book_info = [title, price, rating, stock]\n",
    "\n",
    "    # Append the book information to the main list\n",
    "    book_info_list.append(book_info)\n",
    "\n",
    "# Print the list of lists containing book information\n",
    "for book_info in book_info_list:\n",
    "    print(book_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the `requests` package has been imported we can use the various excellent methods that are built into the package. The most common method for web scraping is the `get` method.\n",
    "\n",
    "`requests.get` will send a `get` request to a web address that you specify.  This simple example will get everything from the web server at that url, but `requests` has powerful tools for selecting exactly what you want to scrape, which we will explore in a later lesson.\n",
    "\n",
    "Try running the code below.\n",
    "What response do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://books.toscrape.com/catalogue/'  \n",
    "\n",
    "# Initialize an empty list to store scraped data\n",
    "book_info_list = []\n",
    "\n",
    "page_number = 1\n",
    "\n",
    "while True:\n",
    "    # Construct the URL for the current page\n",
    "    url = f'{base_url}page-{page_number}.html'\n",
    "    \n",
    "    # Fetch the page content\n",
    "    results = requests.get(url)\n",
    "    \n",
    "    if results.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(results.content, 'html.parser')\n",
    "        \n",
    "        # Find all articles with class 'product_pod'\n",
    "        articles = soup.find_all('article', class_='product_pod')\n",
    "        \n",
    "        # Scraping logic for each article\n",
    "        for article in articles:\n",
    "            # Extract title\n",
    "            title = article.find('h3').find('a')['title']\n",
    "\n",
    "            # Extract product price\n",
    "            price = article.find('p', class_='price_color').text.replace(\"Â\", \"\")\n",
    "            \n",
    "            # Extract star rating (if available)\n",
    "            rating = article.find('p', class_='star-rating')['class'][1]\n",
    "\n",
    "            # Extract stock status\n",
    "            stock = article.find('p', class_='instock availability').text.strip()\n",
    "\n",
    "            # Store the information in a list\n",
    "            book_info = [title, price, rating, stock]\n",
    "\n",
    "            # Append the book information to the main list\n",
    "            book_info_list.append(book_info)\n",
    "        \n",
    "        # Find the 'Next' link\n",
    "        next_link = soup.find('li', class_='next')\n",
    "        \n",
    "        if next_link:\n",
    "            # Update page number for the next iteration\n",
    "            page_number += 1\n",
    "        else:\n",
    "            # No 'Next' link found, exit the loop\n",
    "            break\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_number}. Status code: {results.status_code}\")\n",
    "        break\n",
    "\n",
    "# Process all_book_info as needed (e.g., save to a file, further analysis)\n",
    "# For example, you can print the scraped data:\n",
    "for book_info in book_info_list:\n",
    "    print(book_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('all_book_data.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(book_info_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
